from typing import Tuple, List, Dict, Counter

import numpy as np
import pandas as pd

from data_loading.UtilityEntities import PathContext, Path, NodeType, PathContexts
from preprocessing.context_split import ContextSplit, PickType
from util import ProcessedFolder


# Loads all .csv files generated by AstMiner in np.ndarray
class PathMinerLoader:

    def __init__(self, project_folder: ProcessedFolder, change_entities: pd.Series, change_to_time_bucket: Dict,
                 min_max_count: Tuple[int, int], author_occurrences: Counter,
                 context_splits: List[ContextSplit] = None):
        self._tokens = self._load_tokens(project_folder.tokens_file)
        self._node_types = self._load_node_types(project_folder.node_types_file)
        self._paths = self._load_paths(project_folder.path_ids_file)
        self._labels, self._path_contexts, self._time_buckets, self._context_indices = \
            self._load_path_contexts_files(project_folder.file_changes, change_entities, change_to_time_bucket,
                                           min_max_count, context_splits, author_occurrences)
        self._n_classes = np.max(self._labels) + 1
        self._context_depth = 0 if context_splits is None else len(context_splits)

        entities, counts = np.unique(self._labels, return_counts=True)
        ec = [(c, e) for e, c in zip(entities, counts)]
        for i, (c, e) in enumerate(sorted(ec)):
            print(f'{i}: {e} -> {c} | {c / len(self._labels):.4f}')

    def _load_tokens(self, tokens_file: str) -> np.ndarray:
        return self._series_to_ndarray(
            pd.read_csv(tokens_file, sep=',', index_col='id', usecols=['id', 'value'], squeeze=True)
        )

    def _load_paths(self, paths_file: str) -> np.ndarray:
        paths = pd.read_csv(paths_file, sep=',', index_col='id', usecols=['id', 'nodeTypes'], squeeze=True)
        paths = paths.map(
            lambda nt: Path(
                list(map(int, nt.split()))
            )
        )
        return self._series_to_ndarray(paths)

    def _load_node_types(self, node_types_file: str) -> np.ndarray:
        node_types = pd.read_csv(node_types_file, sep=',', index_col='id', usecols=['id', 'type', 'direction'])
        node_types['nodeType'] = node_types.apply(
            lambda row: NodeType(row['type'], row['direction']),
            axis=1
        )
        return self._series_to_ndarray(node_types['nodeType'])

    @staticmethod
    def _load_path_contexts_files(path_contexts_files: List[str], change_entities: pd.Series,
                                  change_to_time_bucket: Dict, min_max_count: Tuple[int, int],
                                  context_splits: List[ContextSplit], author_occurrences: Counter) \
            -> Tuple[np.ndarray, PathContexts, np.ndarray, List[np.ndarray]]:

        starts, paths, ends = [], [], []
        labels = []
        time_buckets = []
        if context_splits is not None:
            context_indices = [[] for _ in range(len(context_splits))]
        else:
            context_indices = None

        for path_contexts_file in path_contexts_files:
            contexts = pd.read_csv(path_contexts_file, sep=',',
                                   usecols=['changeId', 'pathsCountBefore', 'pathsCountAfter', 'pathsAfter'])
            # Work only with method creations
            contexts = contexts[np.logical_and(contexts['pathsCountBefore'] == 0, contexts['pathsCountAfter'] > 0)]
            path_contexts = contexts['pathsAfter'].fillna('').map(
                lambda ctx: np.array(list(map(PathContext.fromstring, ctx.split(';'))), dtype=np.object)
                if ctx
                else np.array([])
            )
            starts.append(path_contexts.map(
                lambda ctx_array: np.fromiter(map(lambda ctx: ctx.start_token, ctx_array), np.int32,
                                              count=ctx_array.size)
            ).values)

            paths.append(path_contexts.map(
                lambda ctx_array: np.fromiter(map(lambda ctx: ctx.path, ctx_array), np.int32, count=ctx_array.size)
            ).values)

            ends.append(path_contexts.map(
                lambda ctx_array: np.fromiter(map(lambda ctx: ctx.end_token, ctx_array), np.int32, count=ctx_array.size)
            ).values)

            labels.append(contexts['changeId'].map(
                lambda change_id: change_entities.loc[change_id]
            ).values)

            if change_to_time_bucket is not None:
                time_buckets.append(contexts['changeId'].map(
                    lambda change_id: change_to_time_bucket[change_id]
                ).values)

            if context_splits is not None:
                for i in range(len(context_splits)):
                    context_indices[i].append(contexts['changeId'].map(
                        lambda change_id: context_splits[i].change_to_pick_type[change_id]
                        if change_id in context_splits[i].change_to_pick_type else PickType.IGNORED
                    ))

        starts = np.concatenate(starts)
        paths = np.concatenate(paths)
        ends = np.concatenate(ends)
        labels = np.concatenate(labels)
        if change_to_time_bucket is not None:
            time_buckets = np.concatenate(time_buckets)
        if context_splits is not None:
            context_indices = [np.concatenate(c) for c in context_indices]

        label_counts = np.array([author_occurrences[label] for label in labels])
        indices = np.logical_and(min_max_count[0] <= label_counts, label_counts <= min_max_count[1])
        starts = starts[indices]
        paths = paths[indices]
        ends = ends[indices]
        labels = labels[indices]
        if change_to_time_bucket is not None:
            time_buckets = time_buckets[indices]
        if context_splits is not None:
            context_indices = [c[indices] for c in context_indices]

        return labels, PathContexts(starts, paths, ends), time_buckets, context_indices

    @staticmethod
    def _series_to_ndarray(series: pd.Series) -> np.ndarray:
        converted_values = np.empty(max(series.index) + 1, dtype=np.object)
        for ind, val in zip(series.index, series.values):
            converted_values[ind] = val
        return converted_values

    def tokens(self) -> np.ndarray:
        return self._tokens

    def paths(self) -> np.ndarray:
        return self._paths

    def node_types(self) -> np.ndarray:
        return self._node_types

    def labels(self) -> np.ndarray:
        return self._labels

    def path_contexts(self) -> PathContexts:
        return self._path_contexts

    def time_buckets(self):
        return self._time_buckets

    def context_indices(self, depth):
        return self._context_indices[depth]

    def n_classes(self) -> int:
        return self._n_classes

    def context_depth(self) -> int:
        return self._context_depth
